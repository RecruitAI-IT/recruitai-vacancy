import io
import re
import json
import ast
import base64

import httpx
from anthropic import AsyncAnthropic
from anthropic.types import Message

from opentelemetry.trace import SpanKind
from pdf2image import convert_from_bytes

from internal import interface
from pkg.trace_wrapper import traced_method

from .price import *


class AnthropicClient(interface.IAnthropicClient):
    def __init__(
            self,
            tel: interface.ITelemetry,
            api_key: str,
            proxy: str = None,
    ):
        self.tracer = tel.tracer()
        self.logger = tel.logger()

        if proxy:
            transport = httpx.AsyncHTTPTransport(proxy=proxy)
            self.client = AsyncAnthropic(
                api_key=api_key,
                http_client=httpx.AsyncClient(
                    transport=transport,
                    timeout=900
                ),
                max_retries=3
            )
        else:
            self.client = AsyncAnthropic(
                api_key=api_key,
                http_client=httpx.AsyncClient(
                    timeout=900
                ),
                max_retries=3
            )

    @traced_method(SpanKind.CLIENT)
    async def generate_str(
            self,
            history: list,
            system_prompt: str,
            temperature: float = 1.0,
            llm_model: str = "claude-haiku-4-5",
            max_tokens: int = 4096,
            thinking_tokens: int = None,
            enable_caching: bool = True,
            cache_ttl: str = "5m",
            enable_web_search: bool = True,
            max_searches: int = 5,
            images: list[bytes] = None,
            pdf_file: bytes = None,
    ) -> tuple[str, dict]:
        if pdf_file:
            images = self._pdf_to_images(pdf_file)

        messages = self._prepare_messages(history, enable_caching=enable_caching, images=images)

        api_params: dict = {
            "model": llm_model,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "messages": messages
        }

        if system_prompt:
            if enable_caching:
                api_params["system"] = [
                    {
                        "type": "text",
                        "text": system_prompt,
                        "cache_control": {"type": "ephemeral", "ttl": cache_ttl}
                    }
                ]
            else:
                api_params["system"] = system_prompt

        if enable_web_search:
            api_params["tools"] = [{
                "type": "web_search_20250305",
                "name": "web_search",
                "max_uses": max_searches
            }]

        if thinking_tokens is not None and thinking_tokens > 0:
            api_params["thinking"] = {
                "type": "enabled",
                "budget_tokens": thinking_tokens
            }

        completion_response = await self.client.messages.create(**api_params)

        generate_cost = self._calculate_llm_cost(completion_response, llm_model)

        web_search_info = self._extract_web_search_info(completion_response)
        if web_search_info["used"]:
            self.logger.info("Claude Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ» Ğ²ĞµĞ±-Ğ¿Ğ¾Ğ¸ÑĞº", web_search_info)

        llm_response = ""
        for content_block in completion_response.content:
            if content_block.type == "text":
                llm_response += content_block.text
            elif content_block.type == "thinking":
                self.logger.debug("Extended thinking", {"thinking": content_block.thinking})

        return llm_response, generate_cost

    @traced_method()
    async def generate_json(
            self,
            history: list,
            system_prompt: str,
            temperature: float = 1.0,
            llm_model: str = "claude-haiku-4-5",
            max_tokens: int = 4096,
            thinking_tokens: int = None,
            enable_caching: bool = True,
            cache_ttl: str = "5m",
            enable_web_search: bool = True,
            max_searches: int = 5,
            images: list[bytes] = None,
            pdf_file: bytes = None,
    ) -> tuple[dict, dict]:

        llm_response_str, initial_generate_cost = await self.generate_str(
            history,
            system_prompt,
            temperature,
            llm_model,
            max_tokens,
            thinking_tokens,
            enable_caching,
            cache_ttl,
            enable_web_search,
            max_searches,
            images,
            pdf_file
        )

        generate_cost = initial_generate_cost

        try:
            llm_response_json = self._extract_and_parse_json(llm_response_str)
        except Exception:
            llm_response_json, retry_generate_cost = await self._retry_llm_generate(
                history,
                llm_model,
                temperature,
                llm_response_str,
                system_prompt,
                enable_caching,
            )
            generate_cost = {
                'total_cost': round(generate_cost["total_cost"] + retry_generate_cost["total_cost"], 6),
                'input_cost': round(generate_cost["input_cost"] + retry_generate_cost["input_cost"], 6),
                'output_cost': round(generate_cost["output_cost"] + retry_generate_cost["output_cost"], 6),
                'cached_tokens_savings': round(
                    generate_cost["cached_tokens_savings"] + retry_generate_cost["cached_tokens_savings"], 6),
                'details': {
                    'model': llm_model,
                    'tokens': {
                        'total_input_tokens': generate_cost["details"]["tokens"]["total_input_tokens"] +
                                              retry_generate_cost["details"]["tokens"]["total_input_tokens"],
                        'regular_input_tokens': generate_cost["details"]["tokens"]["regular_input_tokens"] +
                                                retry_generate_cost["details"]["tokens"]["regular_input_tokens"],
                        'cached_tokens': generate_cost["details"]["tokens"]["cached_tokens"] +
                                         retry_generate_cost["details"]["tokens"]["cached_tokens"],
                        'output_tokens': generate_cost["details"]["tokens"]["output_tokens"] +
                                         retry_generate_cost["details"]["tokens"]["output_tokens"],
                        'total_tokens': generate_cost["details"]["tokens"]["total_tokens"] +
                                        retry_generate_cost["details"]["tokens"]["total_tokens"]
                    },
                    'costs': {
                        'regular_input_cost': round(
                            generate_cost["details"]["costs"]["regular_input_cost"] +
                            retry_generate_cost["details"]["costs"]["regular_input_cost"], 6),
                        'cached_input_cost': round(
                            generate_cost["details"]["costs"]["cached_input_cost"] +
                            retry_generate_cost["details"]["costs"]["cached_input_cost"], 6),
                        'output_cost': round(
                            generate_cost["details"]["costs"]["output_cost"] +
                            retry_generate_cost["details"]["costs"]["output_cost"], 6)
                    },
                    'pricing': generate_cost["details"]["pricing"]
                }
            }

        self.logger.info("ĞÑ‚Ğ²ĞµÑ‚ Ğ¾Ñ‚ LLM", {"llm_response": llm_response_json})
        return llm_response_json, generate_cost

    def _prepare_messages(
            self,
            history: list,
            enable_caching: bool = True,
            cache_ttl: str = "5m",
            images: list[bytes] = None  # ğŸ†• ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€
    ) -> list:
        if not history:
            return []

        messages = []

        # ĞĞ°Ñ…Ğ¾Ğ´Ğ¸Ğ¼ Ğ¸Ğ½Ğ´ĞµĞºÑ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞ³Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ°
        last_assistant_idx = None
        for i in range(len(history) - 1, -1, -1):
            if history[i]["role"] == "assistant":
                last_assistant_idx = i
                break

        for i, message in enumerate(history):
            # ĞšÑÑˆĞ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ°
            should_cache = (
                    enable_caching
                    and last_assistant_idx is not None
                    and i == last_assistant_idx
            )

            # ğŸ†• Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğº Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞ¼Ñƒ user ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ
            is_last_user_message = (
                    i == len(history) - 1
                    and message["role"] == "user"
                    and images
            )

            if should_cache:
                messages.append({
                    "role": message["role"],
                    "content": [
                        {
                            "type": "text",
                            "text": message["content"],
                            "cache_control": {"type": "ephemeral", "ttl": cache_ttl}
                        }
                    ]
                })
            elif is_last_user_message:
                # ğŸ†• Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ content Ñ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼Ğ¸
                content = []

                # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ²ÑĞµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ
                for img_bytes in images:
                    media_type = self._detect_image_type(img_bytes)
                    base64_image = base64.b64encode(img_bytes).decode('utf-8')

                    content.append({
                        "type": "image",
                        "source": {
                            "type": "base64",
                            "media_type": media_type,
                            "data": base64_image
                        }
                    })

                # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ñ‚ĞµĞºÑÑ‚ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹
                content.append({
                    "type": "text",
                    "text": message["content"]
                })

                messages.append({
                    "role": message["role"],
                    "content": content
                })
            else:
                messages.append({
                    "role": message["role"],
                    "content": message["content"]
                })

        return messages

    def _detect_image_type(self, image_bytes: bytes) -> str:
        """
        ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ñ‚Ğ¸Ğ¿ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ magic numbers (Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¼ Ğ±Ğ°Ğ¹Ñ‚Ğ°Ğ¼ Ñ„Ğ°Ğ¹Ğ»Ğ°)
        """
        if image_bytes.startswith(b'\xff\xd8\xff'):
            return "image/jpeg"
        elif image_bytes.startswith(b'\x89PNG\r\n\x1a\n'):
            return "image/png"
        elif image_bytes.startswith(b'RIFF') and b'WEBP' in image_bytes[:12]:
            return "image/webp"
        elif image_bytes.startswith(b'GIF87a') or image_bytes.startswith(b'GIF89a'):
            return "image/gif"
        else:
            # ĞŸĞ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ JPEG
            self.logger.warning("ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ñ‚Ğ¸Ğ¿ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ image/jpeg")
            return "image/jpeg"

    def _calculate_llm_cost(
            self,
            completion_response: Message,
            llm_model: str
    ) -> dict:
        if llm_model not in CLAUDE_PRICING_TABLE:
            base_model = llm_model.split('-20')[0] if '-20' in llm_model else llm_model
            if base_model not in CLAUDE_PRICING_TABLE:
                return {
                    'error': f'ĞœĞ¾Ğ´ĞµĞ»ÑŒ {llm_model} Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ° Ğ² Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğµ Ñ†ĞµĞ½',
                    'available_models': list(CLAUDE_PRICING_TABLE.keys())
                }
            llm_model = base_model

        pricing = CLAUDE_PRICING_TABLE[llm_model]

        usage = completion_response.usage
        total_input_tokens = usage.input_tokens

        # Claude API Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ ĞºĞµÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ñ…
        cached_tokens = getattr(usage, 'cache_read_input_tokens', 0)
        cache_creation_tokens = getattr(usage, 'cache_creation_input_tokens', 0)
        regular_input_tokens = total_input_tokens - cached_tokens - cache_creation_tokens

        output_tokens = usage.output_tokens

        # Ğ Ğ°ÑÑ‡ĞµÑ‚ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸
        regular_input_cost = (regular_input_tokens / 1_000_000) * pricing.input_price
        cache_creation_cost = (cache_creation_tokens / 1_000_000) * pricing.input_price

        cached_input_cost = 0
        cached_tokens_savings = 0
        if cached_tokens > 0 and pricing.cached_input_price:
            cached_input_cost = (cached_tokens / 1_000_000) * pricing.cached_input_price
            full_price_for_cached = (cached_tokens / 1_000_000) * pricing.input_price
            cached_tokens_savings = full_price_for_cached - cached_input_cost

        web_search_cost = 0
        web_search_requests = 0

        if hasattr(completion_response.usage, 'server_tool_use'):
            server_tool_use = completion_response.usage.server_tool_use
            if hasattr(server_tool_use, 'web_search_requests'):
                web_search_requests = server_tool_use.web_search_requests
                web_search_cost = web_search_requests * 0.01

        total_input_cost = regular_input_cost + cached_input_cost + cache_creation_cost
        output_cost = (output_tokens / 1_000_000) * pricing.output_price
        total_cost = total_input_cost + output_cost + web_search_cost

        result = {
            'total_cost': round(total_cost, 6),
            'input_cost': round(total_input_cost, 6),
            'output_cost': round(output_cost, 6),
            'web_search_cost': round(web_search_cost, 6),
            'cached_tokens_savings': round(cached_tokens_savings, 6),
            'details': {
                'model': llm_model,
                'web_search_requests': web_search_requests,
                'tokens': {
                    'total_input_tokens': total_input_tokens,
                    'regular_input_tokens': regular_input_tokens,
                    'cached_tokens': cached_tokens,
                    'cache_creation_tokens': cache_creation_tokens,
                    'output_tokens': output_tokens,
                    'total_tokens': total_input_tokens + output_tokens
                },
                'costs': {
                    'regular_input_cost': round(regular_input_cost, 6),
                    'cached_input_cost': round(cached_input_cost, 6),
                    'cache_creation_cost': round(cache_creation_cost, 6),
                    'output_cost': round(output_cost, 6)
                },
                'pricing': {
                    'input_price_per_1m': pricing.input_price,
                    'output_price_per_1m': pricing.output_price,
                    'cached_input_price_per_1m': pricing.cached_input_price
                }
            }
        }

        return result

    async def _retry_llm_generate(
            self,
            history: list,
            llm_model: str,
            temperature: float,
            llm_response_str: str,
            system_prompt: str,
            enable_caching: bool = True,
    ) -> tuple[dict, dict]:
        self.logger.warning("LLM Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ»ÑÑ retry", {"llm_response": llm_response_str})

        retry_history = [
            *history,
            {"role": "assistant", "content": llm_response_str},
            {"role": "user",
             "content": "Ğ¯ Ğ¶Ğµ Ğ¿Ñ€Ğ¾ÑĞ¸Ğ» JSON Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚, ĞºĞ°Ğº Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğµ, ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸ Ğ²ÑÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ² JSON Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ Ğ¸Ğ»Ğ¸ Ñ‚Ğ²Ğ¾Ğ¹ JSON Ğ½Ğµ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ñ‹Ğ¹, Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑŒ ĞµĞ³Ğ¾ Ğ½Ğ° Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ğ¾ÑÑ‚ÑŒ"},
        ]

        # ğŸ†• Ğ’ retry Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ ĞĞ• Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‘Ğ¼, Ñ‚Ğ°Ğº ĞºĞ°Ğº Ğ¾Ğ½Ğ¸ ÑƒĞ¶Ğµ Ğ±Ñ‹Ğ»Ğ¸ Ğ² Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞµ
        retry_messages = self._prepare_messages(retry_history, enable_caching=enable_caching, images=None)

        api_params = {
            "model": llm_model,
            "max_tokens": 4096,
            "temperature": temperature,
            "messages": retry_messages
        }

        if system_prompt:
            if enable_caching:
                api_params["system"] = [
                    {
                        "type": "text",
                        "text": system_prompt,
                        "cache_control": {"type": "ephemeral"}
                    }
                ]
            else:
                api_params["system"] = system_prompt

        completion_response = await self.client.messages.create(**api_params)

        generate_cost = self._calculate_llm_cost(completion_response, llm_model)

        llm_response_str = completion_response.content[0].text
        llm_response_json = self._extract_and_parse_json(llm_response_str)

        return llm_response_json, generate_cost

    def _extract_web_search_info(self, completion_response: Message) -> dict:
        web_search_info = {
            "used": False,
            "total_searches": 0,
            "searches": []
        }

        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ usage Ğ´Ğ»Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²
        if hasattr(completion_response.usage, 'server_tool_use'):
            server_tool_use = completion_response.usage.server_tool_use
            if hasattr(server_tool_use, 'web_search_requests'):
                web_search_info["total_searches"] = server_tool_use.web_search_requests
                web_search_info["used"] = server_tool_use.web_search_requests > 0

        # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¸ÑĞºĞ°
        for i, content_block in enumerate(completion_response.content):
            # ĞŸĞ¾Ğ¸ÑĞºĞ¾Ğ²Ñ‹Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ
            if content_block.type == "server_tool_use" and content_block.name == "web_search":
                search_query = content_block.input.get("query", "")

                search_info = {
                    "tool_use_id": content_block.id,
                    "query": search_query,
                    "results": []
                }

                # Ğ˜Ñ‰ĞµĞ¼ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹
                for result_block in completion_response.content[i + 1:]:
                    if (result_block.type == "web_search_tool_result" and
                            result_block.tool_use_id == content_block.id):

                        # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ¾Ğ¸ÑĞºĞ°
                        if hasattr(result_block, 'content') and isinstance(result_block.content, list):
                            for result in result_block.content:
                                if result.type == "web_search_result":
                                    search_info["results"].append({
                                        "title": result.title,
                                        "url": result.url,
                                        "page_age": getattr(result, 'page_age', None)
                                    })
                        break

                web_search_info["searches"].append(search_info)

        return web_search_info

    def _extract_and_parse_json(self, text: str) -> dict:
        match = re.search(r"\{.*\}", text, re.DOTALL)
        json_str = match.group(0)

        try:
            data = json.loads(json_str)
            return data
        except json.JSONDecodeError:
            try:
                data = ast.literal_eval(json_str)
                if isinstance(data, dict):
                    return data
                else:
                    raise ValueError(f"Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ½Ğµ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ĞµĞ¼: {type(data)}")
            except (ValueError, SyntaxError) as e:
                raise

    @traced_method()
    def _pdf_to_images(self, pdf_bytes: bytes) -> list[bytes]:

        images = convert_from_bytes(pdf_bytes, dpi=200)
        images_bytes = []

        for img in images:
            buffer = io.BytesIO()
            img.save(buffer, format='PNG')
            img_data = buffer.getvalue()
            images_bytes.append(img_data)

        return images_bytes
